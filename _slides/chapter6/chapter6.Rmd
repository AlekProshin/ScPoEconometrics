---
title: "ScPoEconometrics6"
author: "Florian Oswald"
date: "`r Sys.Date()`"
output: 
    ioslides_presentation:
        highlight: textmate
        widescreen: true
        logo: ../../images/ScPo.png
        self_contained: false
        css: ../style.css
---

<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Talking about Stars

## What's the meaning of the `***` Stars?  {.columns-2 .smaller}

```{r t1, echo=FALSE}
summary(lm(mpg ~ wt + hp, mtcars))
```
<p class="forceBreak"></p>

>* Up until today, we only looked at column `Estimate`.
>* The remaining three columns refer to the precision of, or our degree of confidence in `Estimate`.
>* Today we'll finally find out why there is *uncertainty* involved in any of this! 
>* `r emo::ji("tada")`


# Statistical Models

## What is a Statistical Model?

<div class="quote-container">
A Statistical Model is a set of assumptions about how the data have been **generated**. It formally describes a *data-generating process* (DGP)
</div>

* Notice the importance of *assumption* in the above definition.
* A Statistical Model is **our** assumption about the DGP. (Assumptions can be more or less appropriate.)

## Classical Model: Minimal Requirements

* The *Classical Regression Model*:
    $$
    y_i = \beta_0 + \beta_1 x_i + \varepsilon_i 
    $$
    Note the $\beta$ instead of the $b$ and the $\varepsilon$ instead of $e$! `r emo::ji("thinking")`

### Minimal Requirements for valid OLS:

* The data are **not linearly dependent**. $\Rightarrow$ *Rank condition*!
* $x$ should be **strictly exogenous** to the model. $E[\varepsilon|x] = 0$
    * This implies $Cov(x,\varepsilon)=0$.

## Additional Assumptions

* The data are drawn from a **random sample** of size $n$.
* The variance of the error term $\varepsilon$ is the same for each value of $x$: $Var(\varepsilon|x) = \sigma^2$. This property is called **homoskedasticity**.

<div class="centered">
<div class="red">
Minimal Requirements + Additional Assumptions => NORMAL Linear Regression Model.
</div>
</div>

## $b_1$ is not $\beta_1$!

>* $b_1$ *estimates* $\beta_1$. All estimates involve **uncertainty**.
>* Ideally, $b_1$ should be close to $\beta_1$.
>* `r emo::ji("bulb")` For each different sample $\{x_i,y_i\}_{i=1}^N$ we'll get a different $b_1$!
>* In general, the greater $N$, the greater the precision of $b_1$.

## Standard Error of $b_1$

>* Under the previous assumptions, we have
    $$
    Var(b_1|x_i) = \frac{\sigma^2}{\sum_i^N (x_i - \bar{x})^2} 
    $$
>* In practice we don't know $\sigma^2$ and *estimate* it with residuals $e_i$:
    $$
    s^2 = \frac{SSR}{n-p} = \frac{\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2}{n-p} =  \frac{\sum_{i=1}^n e_i^2}{n-p}
    $$
    ($n-p$ in small samples). $s^2$ is the *mean squared error*.

## Standard Error of $b_1$

>* Variance is thus
    $$
    Var(b_1|x_i) = \frac{SSR}{(n-p)\sum_i^N (x_i - \bar{x})^2} 
    $$
>* Standard Error (SE):
    $$
    SE(b_1) = \sqrt{Var(b_1|x_i)}
    $$    
>* You clearly see that the variance shrinks as $n\to\infty$.


## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("estimate")
```

## $b_0$ and $b_1$ are *just like* $\bar{x}$! {.build}

* In the app, you see that larger $N$ implies higher precision of $\bar{x}$
* Density becomes *normal* for large $N$.
* Similar for OLS estimates!

## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("standard_errors_simple") 
```


## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("standard_errors_changeN") 
```

# Detour: Sampling

## Tasked by Monsieur le Directeur

# Confidence Intervals

# Hypothesis Testing



# Finally: `***` Stars!

