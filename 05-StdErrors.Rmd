# Standard Errors {#std-errors}

In the previous chapter we have seen how the OLS method can produce estimates about intercept and slope coefficients from data. You have seen this method at work in `R` by using the `lm` function as well. It is now time to introduce the notion that given that $\hat{\beta}_0$ and $\hat{\beta}_1$ are *estimates* of some unkown *population parameters*, there is some degree of **uncertainty** about their values. An other way to say this is that we want some indication about the *precision* of those estimates. 

```{block,type="note"}
<center>
How *confident* should we be about the estimated values $\hat{\beta}_0$ and $\hat{\beta}_1$?
</center>
```
<br>
Let's remind ourselves of the example at the end of the previous chapter, and that we introduced the term *confidence interval*, shown here as the shaded area:

```{r,fig.align="center",message=FALSE,warning=FALSE,echo=FALSE}
library(ggplot2)
library(Ecdat)
p <- ggplot(mapping = aes(x = str, y = testscr), data = Caschool) # base plot
p <- p + geom_point() # add points
p <- p + geom_smooth(method = "lm", size=1, color="red") # add regression line
p <- p + scale_y_continuous(name = "Average Test Score") + 
         scale_x_continuous(name = "Student/Teacher Ratio")
p + theme_bw() + ggtitle("Testscores vs Student/Teacher Ratio")
```
  
The shaded area shows us the region within which the **true** red line will lie with 95% probability. The fact that there is unknown true line (i.e. a *true* slope coefficient $\beta_1$) that we wish to uncover from a sample of data should remind you immediately of our first tutorial. There we wanted to estimate the true population mean from a sample of data, and we saw that as the sample size $N$ increased, our estimate got better and better - fundamentally this is the same idea here.

## What is *true*?

We have a true data-generating process in mind. Let's bring back our simple model \@ref(eq:abline) from the previous chapter:

$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i (\#eq:abline-5)
$$ 

First, we *assume* that this is the correct represenation of the DGP, which looks like the above equation. With that assumption in place, the values $\beta_0$ and $\beta_1$ are the *true parameter values* which generated the data. Notice, that both $\beta_0$ and $\beta_1$ don't have a "hat", which is widely used to indicate an estimate. Now, the fact that our data $(y_i,x_i)$ are a sample from a larger population means that there will be *sampling variation* in our estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ - exactly like in the case of the sample mean estimating the population average as mentioned above.

## Experiencing Standard Errors

We would like to make this point in a purely experiential way, i.e. we want you to experience what is going on. We invite you to spend some time with the following apps, before going into the associated tutorial:

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("standard_errors_simple") # start with this
launchApp("standard_errors_changeN")  # then do that
library(learnr)   # load the learner library
run_tutorial("standared-errors")   # WIP
```

## Standard Errors in Theory

The precise formulae for the standard errors of regression coefficients depend critically on exactly which model we talk about. In other words, certain assumptions about the underlying DGP give rise to certain standard error formulas. What we continue to call the *simple linear regression model* is in fact a list of (at a minimum) three assumptions, and it is time to spell them out:

1. The model is linear in parameters. I.e. it looks like \@ref(eq:abline-5) above.
1. The mean of $\varepsilon$ in \@ref(eq:abline-5) is zero, conditional on $x$. This means that $\varepsilon$ and $x$ should not be correlated, i.e. we require that $Cov(x_i,\varepsilon_i) = 0$.
1. Finally, either it is known that $\varepsilon$ is normally distributed, or we have a sufficiently large sample size $n$.

Assumptions 1. and 3. are mainly of a technical nature. Assumption 2., however, is key for our understanding of the regression machinery in econometrics. 


## What's in a good model?

Imagine model \@ref(eq:abline-5) measures the sales price of houses, and that the explanatory variable $x$ measures the number of rooms in each house sold. You find a positive impact of rooms on houses:

```{r housing}
data(Housing, package="Ecdat")
options(scipen=999)
hlm = lm(price ~ bathrms, data = Housing)
summary(hlm)
```

In fact, from this you conclude that each additional bathroom increases the sales price of a house by `r round(coef(hlm)[2],1)` dollars.



```{r, fig.align='center', fig.cap='Distribution of `lotsize` by `bathrms`'}
options(scipen = 0)
h = subset(Housing,lotsize<13000 & bathrms<4)
h$bathrms = factor(h$bathrms)
ggplot(data=h,aes(x=lotsize,color=bathrms,fill=bathrms)) + geom_density(alpha=0.2,size=1) + theme_bw()
```
