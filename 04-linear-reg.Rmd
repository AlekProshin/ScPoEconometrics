# Linear Regression {#linreg}

Notes:

stop at R-squared
* different data: missing variable
* non-linear realtionship
    
## Data on Cars

We will look at the built-in `cars` dataset. Let's get a view of this by just typing `View(cars)` in Rstudio. You can see something like this:

```{r,echo=FALSE}
head(cars)
```

We have a `data.frame` with two columns: `speed` and `dist`. Type `help(cars)` to find out more about the dataset. There you could read that

>The data give the speed of cars (mph) and the distances taken to stop (ft).

It's good practice to know the extent of a dataset. You could just type 

```{r}
dim(cars)
```

to find out that we have 50 rows and 2 columns. A central question that we want to ask now is the following:

### How are `speed` and `dist` related?

The simplest way to start plot the data. Remembering that we view each row of a data.frame as an observation, we could just label one axis of a graph `speed`, and the other one `dist`, and go through our table above row by row. We just have to read off the x/y coordinates and mark them in the graph. In `R`:

```{r}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
```

Here, each dot represents one observation. In this case, one particular measurement `speed` and `dist` for a car. Now, again: 


```{block, type='note'}
How are `speed` and `dist` related? How could one best *summarize* this relationship?
```

<br>
One thing we could do, is draw a straight line through this scatterplot, like so:

```{r}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 60,b = 0,lw=3)
```

Now that doesn't seem a particularly *good* way to summarize the relationship. Clearly, a *better* line would be not be flat, but have a *slope*, i.e. go upwards:

```{r,echo=FALSE}
plot(dist ~ speed, data = cars,
     xlab = "Speed (in Miles Per Hour)",
     ylab = "Stopping Distance (in Feet)",
     main = "Stopping Distance vs Speed",
     pch  = 20,
     cex  = 2,
     col  = "red")
abline(a = 0,b = 5,lw=3)
```

That is slightly better. However, the line seems at too high a level - the point at which it crosses the y-axis is called the *intercept*; and it's too high. We just learned how to represent a *line*, i.e. with two numbers called *intercept* and *slope*. So how to choose the **best** line?

### Choosing the Best Line

```{r, echo = FALSE, message = FALSE, warning = FALSE}
generate_data = function(int = 0.5,
                         slope = 1,
                         sigma = 10,
                         n_obs = 9,
                         x_min = 0,
                         x_max = 10) {
  x = seq(x_min, x_max, length.out = n_obs)
  y = int + slope * x + rnorm(n_obs, 0, sigma)
  fit = lm(y ~ x)
  y_hat = fitted(fit)
  y_bar = rep(mean(y), n_obs)
  error = resid(fit)
  data.frame(x, y, y_hat, y_bar, error)
}

plot_total_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SST (Sum of Squares Total)", 
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y,
         col = 'grey', lwd = 1, lty = 3, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  # abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_total_dev_prop = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SST (Sum of Squares Total)", 
       xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'dodgerblue', lwd = 1, lty = 2, length = 0.2, angle = 20)
  abline(h = mean(reg_data$y), lwd = 2,col = "grey")
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
}

plot_unexp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, 
       xlab = "x", ylab = "y", pch = 20, cex = 2,asp=1)
  arrows(reg_data$x, reg_data$y_hat,
         reg_data$x, reg_data$y,
         col = 'red', lwd = 2, lty = 1, length = 0.1, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
}

plot_unexp_SSR = function(reg_data) {
  plot(reg_data$x, reg_data$y,
       xlab = "x", ylab = "y", pch = 20, cex = 2, 
  rect(xleft = reg_data$x, ybottom = reg_data$y,
         xright = reg_data$x + abs(reg_data$error), ytop = reg_data$y - reg_data$error, density = -1,
         col = rgb(red = 1, green = 0, blue = 0, alpha = 0.5), border = NA),asp=1)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "black")
}

plot_exp_dev = function(reg_data) {
  plot(reg_data$x, reg_data$y, main = "SSReg (Sum of Squares Regression)", 
  xlab = "x", ylab = "y", pch = 20, cex = 2, col = "grey")
  arrows(reg_data$x, reg_data$y_bar,
         reg_data$x, reg_data$y_hat,
         col = 'darkorange', lwd = 1, length = 0.2, angle = 20)
  abline(lm(y ~ x, data = reg_data), lwd = 2, col = "grey")
  abline(h = mean(reg_data$y), col = "grey")
}
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
set.seed(21)
plot_data = generate_data(sigma = 2)
```

Suppose we have the following set of `r nrow(plot_data)` observations on `x` and `y`, and we put the *best* straight line into it, that we can think of. It looks like this: 

```{r line-arrows, echo=FALSE, message=FALSE, warning=FALSE, fig.cap="The best line and its errors",fig.align="center"}
plot_unexp_dev(plot_data)
```

The red arrows indicate the **distance** of the line to each point and we call them *errors* or *residuals*, often written with the symbol $\varepsilon$. An upward pointing arrow indicates a positive value of a particular $\varepsilon_i$, and vice versa for downward pointing arrows. The name *residual* comes from the way we write an equation for this relationship between two particular values $(y_i,x_i)$ belonging to observation $i$:



$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i (\#eq:abline)
$$ 

Here $\beta_0$ is the intercept, and $\beta_1$ is the slope of our line, and $\varepsilon_i$ is the value of the arrow (i.e. a positive or negative number) indicating the distance between the actual $y_i$ and what is predicted by our line. In other words, $\varepsilon_i$ is what is left to be explained on top of the line $\beta_0 + \beta_1 x_i$, hence, it's a residual to explain $y_i$. Now, back to our claim that this is the *best* line. What exactly characterizes the best line?

```{block,type="warning"}
<center>
The best line minimizes the sum of **squared residuals**, i.e. it minimizes the SSR: $$ \varepsilon_1^2 + \dots + \varepsilon_N^2 = \sum_{i=1}^N \varepsilon_i^2 \equiv \text{SSR}$$
</center>
```

<br>
Wait a moment, why *squared* residuals? This is easy to understand: suppose that instead, we wanted to just make the *sum* of the arrows in figure \@ref(fig:line-arrows) as small as possible (that is, no squares). Choosing our line to make this number small would not give a particularly good representation of the data -- given that errors of opposite sign and equal magnitude offset, we could have very long arrows (but of opposite signs), and a poor resulting line. Squaring each error avoids this (because now negative errors get positive values!)
We illustrate this in figure \@ref(fig:line-squares). This is the same data as in figure \@ref(fig:line-arrows), but instead of arrows of length $\varepsilon_i$ for each observation $i$, now we draw a square with side $\varepsilon_i$, i.e. an area of $\varepsilon_i^2$. You will see in the practical sessions that choosing a different line to this one will increase the sum of squares.

```{r line-squares, echo=FALSE, message=FALSE, warning=FALSE, fig.align='center', fig.cap="The best line and its SQUARED errors"}
plot_unexp_SSR(plot_data)
```

### Ordinary Least Squares (OLS) Coefficients

The method to estimate $\beta_0$ and $\beta_1$ we illustrated above is called *Ordinary Least Squares*, or OLS. There is a connection between the estimate for $\beta_1$ - denoted $\hat{\beta}_1$ - in equation \@ref(eq:abline) and the *covariance* of $y$ and $x$ - remember how we defined this in section \@ref(summarize-two). In the simple case shown in equation \@ref(eq:abline), the relationship is

$$
\hat{\beta}_1 = \frac{cov(x,y)}{var(x)}.  (\#eq:beta1hat)
$$
i.e. the estimate of the slope coefficient is the covariance between $x$ and $y$ divided by the variance of $x$. Similarly, the estimate for the intercept is given by

$$
\hat{\beta}_0 = \bar{y} - \hat{\beta}_1 \bar{x}.  (\#eq:beta0hat)
$$

where $\bar{z}$ denotes the sample mean of variable $z$.

### Correlation, Covariance and Linearity

It is important to keep in mind that Correlation and Covariance relate to a *linear* relationship between `x` and `y`. Given how the regression line is estimated by OLS (see just above), you can see that the regression line inherits this property from the Covariance. 
A famous exercise by Francis Anscombe (1973) illustrates this by constucting 4 different datasets which all have identical **linear** statistics: mean, variance, correlation and regression line *are identical*. However, the usefulness of the statistics to describe the relationship in the data is not clear.

```{r,echo=FALSE}
##-- now some "magic" to do the 4 regressions in a loop:
ff <- y ~ x
mods <- setNames(as.list(1:4), paste0("lm", 1:4))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  ## or   ff[[2]] <- as.name(paste0("y", i))
  ##      ff[[3]] <- as.name(paste0("x", i))
  mods[[i]] <- lmi <- lm(ff, data = anscombe)
}

op <- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))
for(i in 1:4) {
  ff[2:3] <- lapply(paste0(c("y","x"), i), as.name)
  plot(ff, data = anscombe, col = "red", pch = 21, bg = "orange", cex = 1.2,
       xlim = c(3, 19), ylim = c(3, 13),main=paste("dataset",i))
  abline(mods[[i]], col = "blue")
}
par(op)
```

The important lesson from this example is the following:

```{block,type="warning"}
<center>
Always **visually inspect** your data, and don't rely exclusively on summary statistics like *mean, variance, correlation and regression line*. All of those assume a **linear** relationship between the variables in your data.
</center>
```

### Non-Linear Relationships in Data

Suppose our data now looks like this:

```{r non-line-cars,echo=FALSE}
with(mtcars,plot(hp,mpg,xlab="x",ylab="y"))
```

Putting our previous *best line* defined in equation \@ref(eq:abline) as $y = \beta_0 + \beta_1 x + u$, we get something like this:

```{r non-line-cars-ols,echo=FALSE,fig.align='centre',fig.cap='Best line with non-linear data?'}
l1 = lm(mpg~hp,data=mtcars)
plot(mtcars$hp,mtcars$mpg,xlab="x",ylab="y")
abline(reg=l1,lw=2)
```

Somehow when looking at \@ref(fig:non-line-cars-ols) one is not totally convinced that the straight line is a good summary of this relationship. For values $x\in[50,120]$ the line seems to low, then again too high, and it completely misses the right boundary. It's easy to address this shortcoming by including *higher order terms* of an explanatory variable. We would modify \@ref(eq:abline) to read now

$$
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \varepsilon_i (\#eq:abline2)
$$ 

This is a special case of *multiple regression*, which we will talk about in chapter \@ref(multiple-reg). You can see that there are *multiple* slope coefficients. For now, let's just see how this performs:

```{r non-line-cars-ols2,echo=FALSE,fig.align="centre",fig.cap="Better line with non-linear data!",echo=FALSE}
l1 = lm(mpg~hp+I(hp^2),data=mtcars)
newdata=data.frame(hp=seq(from=min(mtcars$hp),to=max(mtcars$hp),length.out=100))
newdata$y = predict(l1,newdata=newdata)
plot(mtcars$hp,mtcars$mpg,xlab="x",ylab="y")
lines(newdata$hp,newdata$y,lw=2)
```
