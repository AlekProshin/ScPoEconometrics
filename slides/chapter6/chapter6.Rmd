---
title: "ScPoEconometrics6"
author: "Florian Oswald"
date: "`r Sys.Date()`"
output: 
    ioslides_presentation:
        highlight: textmate
        widescreen: true
        logo: ../../images/ScPo.png
        self_contained: false
        css: ../style.css
---

<style>
.forceBreak { -webkit-column-break-after: always; break-after: column; }
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Talking about Stars

## What's the meaning of the `***` Stars?  {.columns-2 .smaller}

```{r t1, echo=FALSE}
summary(lm(mpg ~ wt + hp, mtcars))
```
<p class="forceBreak"></p>

>* Up until today, we only looked at column `Estimate`.
>* The remaining three columns refer to the precision of, or our degree of confidence in `Estimate`.
>* Today we'll finally find out why there is *uncertainty* involved in any of this! 
>* `r emo::ji("tada")`


# Statistical Models

## What is a Statistical Model?

<div class="quote-container">
A Statistical Model is a set of assumptions about how the data have been **generated**. It formally describes a *data-generating process* (DGP)
</div>

* Notice the importance of *assumption* in the above definition.
* A Statistical Model is **our** assumption about the DGP. (Assumptions can be more or less appropriate.)

## Classical Model: Minimal Requirements

* The *Classical Regression Model*:
    $$
    y_i = \beta_0 + \beta_1 x_i + \varepsilon_i 
    $$
    Note the $\beta$ instead of the $b$ and the $\varepsilon$ instead of $e$! `r emo::ji("thinking")`

### Minimal Requirements for valid OLS:

* The data are **not linearly dependent**. $\Rightarrow$ *Rank condition*!
* $x$ should be **strictly exogenous** to the model. $E[\varepsilon|x] = 0$
* This implies $Cov(x,\varepsilon)=0$.

## Additional Assumptions

* The data are drawn from a **random sample** of size $n$.
* The variance of the error term $\varepsilon$ is the same for each value of $x$: $Var(\varepsilon|x) = \sigma^2$. This property is called **homoskedasticity**.

## Standard Error of $b_1$

>* Under the previous assumptions, we have
    $$
    Var(b_1|x_i) = \frac{\sigma^2}{\sum_i^N (x_i - \bar{x})^2} =     \frac{\sigma^2}{Var(x)}
    $$
>* In practice we don't know $\sigma^2$ and *estimate* it with residuals $e_i$:
    $$
    s^2 = \frac{SSR}{n} = \frac{\sum_{i=1}^n (y_i - b_0 - b_1 x_i)^2}{n} =  \frac{\sum_{i=1}^n e_i^2}{n}
    $$
    ($n-p$ in small samples). $s^2$ is the *mean squared error*.

## Standard Error of $b_1$

>* Variance is thus
    $$
    Var(b_1|x_i) = \frac{1}{n}\frac{SSR}{Var(x)}
    $$
>* You clearly see that the variance shrinks as $n\to\infty$.


## $b_0$ and $b_1$ are *Estimates*

>* Previously, we repeatedly saw 
    $$ y_i = b_0 + b_1 x_i + e_i $$
>* `R` reports *estimates* $b_0$ and $b_1$. Both estimate underlying *population parameters* $\beta_0$ and $\beta_1$
>* Estimates **always** come with some degree of uncertainty.
>* $Var(b_1|x_i)$ measures how much uncertainty.
>* `r emo::ji("bulb")` For each different sample $\{x_i,y_i\}_{i=1}^n$ we'll get a different $b_1$!

## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("estimate")
```

## $b_0$ and $b_1$ are *just like* $\bar{x}$! {.build}

* In the app, you see that larger $N$ implies higher precision of $\bar{x}$
* Density becomes *normal* for large $N$.
* Similar for OLS estimates!

## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("standard_errors_simple") 
```


## App!

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("standard_errors_changeN") 
```


# Finally: `***` Stars!

## Regression Output {.smaller}

1. Now we know what `Std. Error` means.
2. What about the rest?

```{r t2, echo=FALSE}
l1 = lm(mpg ~ wt + hp, mtcars)
summary(l1)
```


## Student's T-Distribution and Testing

>* Similar in shape to the Normal Distribution (in large samples: identical)
>* If $\varepsilon \sim \mathcal{N}(0,\sigma^2)$, then $b_k / sd(b_k)$ follows the T distribution with $N-K$ degrees of freedom.
>* This is used to test the **hypothesis** that true value in the population is $\beta_k = 0$.
>* If we cannot *reject* $\beta_k = 0$, we cannot be sure that $x_k$ should even be included in our regression.
