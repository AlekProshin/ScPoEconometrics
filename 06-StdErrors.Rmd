# Standard Errors {#std-errors}

In the previous chapters we have seen how the OLS method can produce estimates about intercept and slope coefficients from data. You have seen this method at work in `R` by using the `lm` function as well. It is now time to introduce the notion that given that $\hat{\beta}_0$, $\hat{\beta}_1$ and $\hat{\beta}_2$ are *estimates* of some unkown *population parameters*, there is some degree of **uncertainty** about their values. An other way to say this is that we want some indication about the *precision* of those estimates. 

```{block,type="note"}
<center>
How *confident* should we be about the estimated values $\hat{\beta}$?
</center>
```
<br>
Let's go back to the regression with one variable and remind ourselves of the example at the end of chapter \@ref(linreg). There we introduced the term *confidence interval*, shown here as the shaded area:

```{r,fig.align="center",message=FALSE,warning=FALSE,echo=FALSE}
library(ggplot2)
library(Ecdat)
p <- ggplot(mapping = aes(x = str, y = testscr), data = Caschool) # base plot
p <- p + geom_point() # add points
p <- p + geom_smooth(method = "lm", size=1, color="red") # add regression line
p <- p + scale_y_continuous(name = "Average Test Score") + 
         scale_x_continuous(name = "Student/Teacher Ratio")
p + theme_bw() + ggtitle("Testscores vs Student/Teacher Ratio")
```
  
The shaded area shows us the region within which the **true** red line will lie with 95% probability. The fact that there is unknown true line (i.e. a *true* slope coefficient $\beta_1$) that we wish to uncover from a sample of data should remind you immediately of our first tutorial. There, we wanted to estimate the true population mean from a sample of data, and we saw that as the sample size $N$ increased, our estimate got better and better - fundamentally this is the same idea here.

## What is *true*?

We have a true data-generating process in mind. Let's bring back our simple model \@ref(eq:abline):

$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i (\#eq:abline-5)
$$ 

First, we *assume* that this is the correct represenation of the DGP, which looks like the above equation. With that assumption in place, the values $\beta_0$ and $\beta_1$ are the *true parameter values* which generated the data. Notice, that both $\beta_0$ and $\beta_1$ do **not** have a "hat", which is widely used to indicate an estimate. Now, the fact that our data $(y_i,x_i)$ are a sample from a larger population means that there will be *sampling variation* in our estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ - exactly like in the case of the sample mean estimating the population average as mentioned above. In general, the more observations we have the greater the precision of our estimates.


## The Classical Regression Model {#class-reg}

The smallest set of assumptions used to define the *classical regression model* are the following:

1. The data are **not linearly dependent**: Each variable provides new information for the outcome, and it cannot be replicated as a linear combination of other variables. For example, suppose we have two variables for each individual, $x_1,x_2$ which determine an outcome $y$. If $x_{2} = 2x_{1}$, then $x_2$ provides no new information. It's enough to know $x_1$, and multiply it by 2. In this sense, $x_2$ is a redundant variable and should not be included in the model.
1. The mean of the residuals conditional on $x$ should be zero, $E[\varepsilon|x] = 0$. Notice that this also means that $Cov(\varepsilon,x) = 0$, i.e. that the errors and our explanatory variable(s) should be *uncorrelated*. It is said that $x$ should be **strictly exogenous** to the model.

These assumptions are necessary to successfully (and correctly!) run an OLS regression. They are often supplemented with an additional set of assumptions, which help with certain aspects of the exposition, but are not strictly necessary:

3. The data are drawn from a **random sample** of size $n$: observation $(x_i,y_i)$ comes from the exact same distribution, and is independent of observation $(x_j,y_j)$, for all $i\neq j$.
4. The variance of the error term $\varepsilon$ is the same for each value of $x$: $Var(\varepsilon|x) = \sigma^2$. This property is called **homoskedasticity**.

### Standard Errors in Theory

Under assumptions 1. through 4. we can define the formula for the standard error of our slope coefficient in the context of our single regressor model \@ref(eq:abline-5) as follows: 

$$
Var(\beta_1|x_i) = \frac{\sigma^2}{x_1^2 + x_2^2 + \dots + x_n^2}  (\#eq:var-ols)
$$
In pratice, we don't know the theoretical variance of $\varepsilon$, but we form an estimate about it from our sample of data. A widely used estimate uses the already encountered SSR (sum of squared residuals), and is denoted $s^2$:

$$
s^2 = \frac{SSR}{n-p} = \frac{\sum_{i=1}^n (y_i - \hat{\beta_0} - \hat{\beta_1}x_i)^2}{n-p}
$$
where $n-p$ are the *degrees of freedom* available in this estimation. $p$ is the number of parameters we wish to estimate (here: 1). So, the variance formula would become

$$
Var(\beta_1|x_i) = \frac{SSR}{(n-p)(x_1^2 + x_2^2 + \dots + x_n^2)}  (\#eq:var-ols2)
$$

You can clearly see that, as $n$ increases, the denominator increases, and therefore the variance of the estimate will decrease.

### Standard Errors in Practice

We would like to further make this point in an experiential way, i.e. we want you to experience what is going on. We invite you to spend some time with the following apps, before going into the associated tutorial:

```{r,eval=FALSE}
library(ScPoEconometrics)
launchApp("sampling")  
launchApp("standard_errors_simple") 
launchApp("standard_errors_changeN")  # then do that
```

## What's in my model? (And what is not?)

We want to revisit the underlying assumptions of the classical model outlined in \@ref(class-reg). Right now we to talk a bit more about assumption number 2 of the above definition in \@ref(class-reg). It said this:

```{block type='warning'}
The mean of the residuals conditional on $x$ should be zero, $E[\varepsilon|x] = 0$. This means that $Cov(\varepsilon,x) = 0$, i.e. that the errors and our explanatory variable(s) should be *uncorrelated*. We want $x$ to be **strictly exogenous** to the model.
```
<br>
Great. But what does this *mean*? How could $x$ be correlated with something we don't even observe?! Good questions - let's try with an example.

Imagine that this model

$$
y_i = \beta_0 + \beta_1 x_i + \varepsilon_i 
$$ 

is used to measures the impact of number of bathrooms ($x$) on the sales price of houses ($y$). You find a positive impact of bathrooms on houses:

```{r housing,echo=FALSE}
data(Housing, package="Ecdat")
hlm = lm(price ~ bathrms, data = Housing)
summary(hlm)

```

In fact, from this you conclude that each additional bathroom increases the sales price of a house by `r options(scipen=999);round(coef(hlm)[2],1)` dollars. Let's see if our assumption $E[\varepsilon|x] = 0$ is satisfied:

```{r,warning=FALSE,message=FALSE}
library(dplyr)
# add residuals to the data
Housing$resid <- resid(hlm)
Housing %>%
  group_by(bathrms) %>%
  summarise(mean_of_resid=mean(resid))
```

Oh, that doesn't look good. Even though the unconditional mean $E[\varepsilon] = 0$ is *very* close to zero (type `mean(resid(hlm))`!), this doesn't seem to hold at all by categories of $x$. This indicates that there is something in the error term $\varepsilon$ which is *correlated* with `bathrms`. Going back to our discussion about *ceteris paribus* in section \@ref(ceteris), we stated that the interpretation of our OLS slope estimate is that 

```{block,type="tip"}
Keeping everything else fixed at the current value, what is the impact of $x$ on $y$? *Everything* also includes things in $\varepsilon$!
```
<br>

Well that's the problem then. `r options(scipen=999)`We said above that one more bathroom is worth `r round(coef(hlm)[2],1)` dollars - if **nothing else changes**! But that doesn't seem to hold, because we have seen that as we increase `bathrms` from `1` to `2`, the mean of the resulting residuals changes quite a bit. So there **is something in $\varepsilon$ which does change**, hence, our conclusion that one more bathroom is worth `r round(coef(hlm)[2],1)` dollars is in fact *invalid*!

### Omitted Variable Bias

What we are discussing here is called *Omitted Variable Bias*. There is a variable which we omitted from our regression, i.e. we forgot to include it. It is often difficult to find out what that variable could be, and you can go a long way by just reasoning about the data-generating process. In other words, do you think it's *reasonable* that price be determined by the number of bathrooms only? Or could there be another variable, omitted from our model, that is important to explain prices, and at the same time correlated with `bathrms`? 

Let's try with `lotsize`, i.e. the size of the area on which the house stands. Intuitively, larger lots should command a higher price; At the same time, however, larger lots imply more space, hence, you can also have more bathrooms! Let's check this out:


```{r,echo=FALSE}
options(scipen=0)
hlm2 = update(hlm, . ~ . + lotsize)
summary(hlm2)
options(scipen=999)
```

Here we see that the estimate for the effect of an additional bathroom *decreased* from `r round(coef(hlm)[2],1)` to `r round(coef(hlm2)[2],1)` by almost 5000 dollars! The way in which `bathrms` and `lotsize` are correlated is important here, so let's investigate that:


```{r, fig.align='center', fig.cap='Distribution of `lotsize` by `bathrms`',echo=FALSE}
options(scipen=0)
h = subset(Housing,lotsize<13000 & bathrms<4)
h$bathrms = factor(h$bathrms)
ggplot(data=h,aes(x=lotsize,color=bathrms,fill=bathrms)) + geom_density(alpha=0.2,size=1) + theme_bw()
```

This shows that lotsize and the number of bathrooms is indeed positively related. Larger lot of the house, more bathrooms. This leads to a general result:

```{block type='note'}
**Direction of Omitted Variable Bias**

If there is an omitted variable $z$ that is *positively* correlated with our explanatory variable $x$, then our estimate of effect of $x$ on $y$ will be too large (or, *biased upwards*). The correlation between $x$ and $z$ means that we attribute part of the impact of $z$ on $y$ mistakenly to $x$! And, of course, vice versa for *negatively* correlated omitted variables.
```
<br>


