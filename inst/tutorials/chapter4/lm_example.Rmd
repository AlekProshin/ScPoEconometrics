---
title: "Linear Regression in `R`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Luckily for us, fitting a linear model to some data does not require us to iteratively find the best intercept and slope manually. As it turns out, `R` can do this much more precisely, and very fast!

Let's explore how to do this, using a real life dataset taken from the `Ecdat` package which includes many economics-related dataset. In this example, we will use the `Caschool`dataset which contains the average test scores of 420 elementary schools in California along with some additional information.

## Loading and exploring Data

We can explore which variables are included in the dataset using the `names()` function:

```{r str, warning=F, message = F}
library("Ecdat") # Attach the Ecdat library

names(Caschool) # Display the variables of the Caschool dataset
```

For each variable in the dataset, basic summary statistics can be obtained by calling `summary()`

```{r summary}
summary(Caschool[, c("testscr", "str", "avginc")])
```


## Fitting a linear model to the data.

Suppose a policymaker is interested in the following linear model:

$$testscr_i = \beta_0 + \beta_1 \times (str)_i + \epsilon_i$$
Where $(testscr)_i$ is the *average test score* for a given school $i$ and $(str)_i$ is the *Student/Teacher Ratio* (i.e. the average number of students per teacher) in the same school $i$. We can think of $\beta_0$  and $\beta_1$ as the intercept and the slope of the regression line.

The subscript $i$ indexes all unique elementary schools ($i \in \{1, 2, 3, \dots 420\}$) and $\epsilon_i$ is the error, or *residual*, of the regression. (Remember that our procedure for finding the line of best fit is to minimize the *sum of squared residuals* (SSR)).

----

At this point you should step back and take a second to think about what you believe the relation between a school's test scores and student/teacher ratio will be. Do you believe that, in general, a high student/teacher ratio will be associated with higher-than-average test scores for the school? Do you think that the number of students per teacher will impact results in any way? 

Let's find out! As always, we will start by plotting the data to inspect it visually (don't worry if the syntax doesn't make much sense right now, we will come back to it very soon):

```{r first_plot}

plot(formula = testscr ~ str,
     data = Caschool,
     xlab = "Student/Teacher Ratio",
     ylab = "Average Test Score", pch = 21, col = 'blue')

```

Can you spot a trend in the data? According to you, what would the line of best fit look like? Would it be upward or downward slopping? Let's ask `R`!

## The `lm()` function

We will use the built-in `lm()` function to estimate the coefficients $\beta_0$ and $\beta_1$ using the data at hand. The `lm()` function typically only takes 2 arguments:

`lm(formula, data)`

- `formula` is the description of our model which we want `R` to estimate for us. Its syntax is very simple: `Y ~ X` (more generally, `DependentVariable ~ Independent Variables`). You can think of the tilda operator `~` as the equal sign in your model equation. An intercept is included by default and so you do not have to ask for it in `formula`.
  For example, the simple model $income = \beta_0 + \beta_1 \cdot age$ can be written as `income ~ age`. You can also ask `R` to estimate a multivariate regression such as $income = \beta_0 + \beta_1 \cdot age + \beta_2 \cdot isWoman$ by simply separating all variables on the right-hand side of the equation with the `+` operator, like this : `income ~ age + isWoman`. A `formula` can sometimes be written between quotation marks: `"X ~ Y"`.

- `data` is simply the `data.frame` containing the variables in the model.

In the context of our example, the function call is therefore:

```{r lmfit}
lm(formula = "testscr ~ str",
   data = Caschool)
```

As we can see, `R` automatically spits out its estimates for the Intercept and Slope coefficients, $\hat{\beta_0} =$ `r round(lm(testscr ~ str, Caschool)$coefficients[1], 2)` and $\hat{\beta_1} =$ `r round(lm(testscr ~ str, Caschool)$coefficients[2], 2)`. The relationship between a school's Student/Teacher Ratio and its average test results is **negative**.

For reasons that are outside the scope of this class, running a linear regression in `R` is typically a two-steps process. You first assign the output of the `lm()` call to an object and **then** call a second function (for our purpose, mainly `summary()`) on the resulting object. In practice, this looks like this :


```{r lmfit_2_steps}
# assign lm() output to some object `fit_california`
fit_california <- lm(formula = "testscr ~ str", data = Caschool)

# ask R for the regression summary
summary(fit_california) 
```

Again, we recognize our intercept and slope estimates from before, alongside some other numbers and indications. This output is called a *regression table*, and you will be able to decypher it by the end of this course.

## Plotting the regression line

We can also use our `lm` fit to draw the regression line on top of our initial scatterplot, using the following syntax:

```{r plot_reg}

plot(formula = testscr ~ str,
     data = Caschool,
     xlab = "Student/Teacher Ratio",
     ylab = "Average Test Score", pch = 21, col = 'blue')# same plot as before

abline(fit_california, col = 'red') # add regression line

```


As you probably expected, there is a slight downward correlation between a school's Student/Teacher Ratio and its average test results.

## Multivariate Regression

With these tools in place, it is fairly straigtforward to estimate models with more than one dependent variables, for example adding families' average income to our previous model:

$$testscr_i = \beta_0 + \beta_1  (str)_i + \beta_2  (avginc)_i + \epsilon_i$$
We can incoporate this new variable to our model by simply adding it to our `formula`:

```{r lmfit_multiv}
fit_multivariate <- lm(formula = "testscr ~ str + avginc", data = Caschool)
summary(fit_multivariate)
```

Although it is quite cumbersome and not typical to visualize multivariate regressions, we can still do this with 2 explanatory variables using a *regression (2-dimensional) plane*.

```{r 3DPlotly, echo = FALSE, warning=F, message = F}
library(plotly)
library(reshape2)

to_plot_x <- c(min(Caschool$str), max(Caschool$str))
to_plot_y <- c(min(Caschool$avginc), max(Caschool$avginc))

df <- data.frame(str = rep(to_plot_x, 2),
           avginc = rep(to_plot_y, each = 2))
df["pred"] <- predict.lm(fit_multivariate, df, se.fit = F)

surf <- acast(df, avginc ~ str)

Caschool %>%
  plot_ly() %>%
  add_markers(x = ~str, y = ~avginc, z = ~testscr, name = "Data", hoverinfo = "skip", opacity = .75, marker=list(color = 'green', size = 5)) %>%
  add_surface(x = to_plot_x, y = to_plot_y, z = ~surf, inherit = F, name = "Best Fit Plane", opacity = .75) %>%
  layout(title =  "Visual Representation of the\nMultivariate Regression")

```

While you explore this plot, ask yourself the following question: if you could only choose one of the two explanatory variables in our model (that is, either $str$ or $avginc$) to predict the value of a given school's average test score, which one would you choose? Why? Discuss this with your classmates.

